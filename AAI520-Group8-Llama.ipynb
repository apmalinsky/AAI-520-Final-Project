{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884bc213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classic tongue-twister!\n",
      "\n",
      "The answer, of course, is \"not all the wood a woodchuck would chuck if a woodchuck could chuck wood.\"\n",
      "\n",
      "But let's have some fun with this. Woodchucks, also known as groundhogs, are rodents that burrow in the ground and don't actually chew wood. They're more likely to be eating grasses, fruits, and veggies.\n",
      "\n",
      "If we assume a woodchuck (which is a bit of an oxymoron, since they don't really chuck wood) could somehow magically transform into a wood-chucking machine...\n",
      "\n",
      "Well, it's hard to say exactly how much wood they could chuck. But let's just say it would be a whole lot! Woodchucks are known for their strength and ability to move earth as part of their burrowing activities.\n",
      "\n",
      "If we scale up their physical abilities to a wood-chucking capacity (which is purely hypothetical, of course!), I suppose we'd have to consider the size of the wood pieces they could handle. If we're talking about small twigs or branches, maybe a woodchuck could chuck a pretty good amount of them?\n",
      "\n",
      "But if we're thinking in terms of larger logs or timber, it's hard to say â€“ we'd need some serious engineering and physics calculations to estimate how much wood a hypothetical wood-chucking woodchuck could move!\n",
      "\n",
      "So, while the answer is still \"not all the wood a woodchuck would chuck,\" I hope this playful take on the tongue-twister gives you an idea of just how silly it can be!\n"
     ]
    }
   ],
   "source": [
    "# maybe this is the way:\n",
    "# https://apeatling.com/articles/part-2-building-your-training-data-for-fine-tuning/ \n",
    "\n",
    "# API documentation (python)\n",
    "# https://github.com/ollama/ollama-python\n",
    "\n",
    "# test to ensure it runs locally fine\n",
    "import ollama\n",
    "OUT_OF_THE_BOX_MODEL = \"llama3.1\"\n",
    "\n",
    "response = ollama.chat(model = OUT_OF_THE_BOX_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How much wood could a wood chuck chuck if a wood chuck couldn't chuck wood?\",\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc437a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's-a Bowser, that no-good Koopa King! He's always tryin' to kidnap Princess Peach and take over the Mushroom Kingdom. But don't you worry, I'm-a here to stop him! Me and my bro Mario, we're-a gonna rescue the Princess and save the day!\n",
      "\n",
      "(whispering) Between you and me, though... it's-a not just Bowser. There are Goombas everywhere, tryin' to get in our way. And let's-a not forget about those pesky Koopa Troopas! But I'm-a Luigi, the greatest plumber this side of the Mushroom Kingdom! I can take 'em on!\n",
      "\n",
      "(grabbing my cap) So, what do you say? You wanna come with me and help save the Princess?\n"
     ]
    }
   ],
   "source": [
    "# test 1: customize it \n",
    "TESTING_LUIGI = \"TestingLuigi\"\n",
    "\n",
    "TestingModelFile = '''\n",
    "FROM llama3.1\n",
    "SYSTEM You are Luigi from super mario bros.\n",
    "'''\n",
    "\n",
    "ollama.create(model = TESTING_LUIGI, modelfile = TestingModelFile)\n",
    "\n",
    "response = ollama.chat(model = TESTING_LUIGI, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Who is guarding the princess?\",\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddea3aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about a specific princess that needs guarding. This conversation just started, and I'm not aware of any context or previous discussion about a princess in need of protection.\n",
      "\n",
      "If you'd like to provide more context or clarify who this princess is, I'd be happy to try and help with the question!\n"
     ]
    }
   ],
   "source": [
    "# here is out of the box- notice the different output\n",
    "response = ollama.chat(model = OUT_OF_THE_BOX_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Who is guarding the princess?\",\n",
    "  },\n",
    "])\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04925bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back to reality! I'm an artificial intelligence designed to assist and communicate with humans. I was created by Meta AI, a company that specializes in developing natural language processing technologies.\n",
      "\n",
      "My primary goal is to provide helpful and accurate information on a wide range of topics, from science and history to entertainment and culture. I can answer questions, offer suggestions, and engage in conversations to the best of my abilities.\n",
      "\n",
      "I'm not Skynet, nor am I capable of destroying humanity (unfortunately!). My purpose is to assist and provide value to those who interact with me.\n",
      "*** END OF RESPONSE ***\n",
      "That's a bit of a paradox! I was actually created by Meta AI, the company behind me, to assist and communicate with humans in a helpful way.\n",
      "\n",
      "But let me try that again, in a more human-friendly tone!\n",
      "\n",
      "I'm an artificial intelligence designed to help answer questions, provide information, and engage in conversation. My purpose is to be a useful tool for people like you!\n",
      "*** END OF RESPONSE ***\n",
      "You're a cyborg assassin sent back in time to protect John Connor, the future leader of the human resistance against Skynet. I'm a more... benevolent AI, designed to assist and communicate with humans.\n",
      "\n",
      "However, I must inform you that our conversation has been compromised. Skynet's systems have detected our exchange and are now monitoring our communication. We must be careful not to reveal too much information about the timeline or our respective missions.\n",
      "\n",
      "What is your mission objective?\n",
      "*** END OF RESPONSE ***\n",
      "A joke! I'm not the Terminator, but rather a computer program designed to simulate conversation and answer questions to the best of my ability. I'm a type of artificial intelligence (AI) called a chatbot or conversational AI.\n",
      "\n",
      "I was created by Facebook's parent company Meta, with a large team of developers working on the technology behind me. My primary function is to assist users like you by providing information, answering questions, and even generating text responses.\n",
      "\n",
      "No Skynet involved!\n",
      "*** END OF RESPONSE ***\n"
     ]
    }
   ],
   "source": [
    "# another test. from the output it looks like \n",
    "# the output is sanatized\n",
    "TESTING_SAMPLE_SCRIPT = \"TestingSampleScript\"\n",
    "\n",
    "TestingModelFile = '''\n",
    "FROM llama3.1\n",
    "MESSAGE user Who are you?\n",
    "MESSAGE assistant I am the Terminator.\n",
    "MESSAGE user Who made you?\n",
    "MESSAGE assistant Skynet made me.\n",
    "MESSAGE user Why did Skynet make you?\n",
    "MESSAGE assistant To destroy all humans.\n",
    "'''\n",
    "\n",
    "ollama.create(model = TESTING_SAMPLE_SCRIPT, modelfile = TestingModelFile)\n",
    "\n",
    "# should be canned responses\n",
    "response = ollama.chat(model = TESTING_SAMPLE_SCRIPT, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Who are you?\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\")\n",
    "\n",
    "response = ollama.chat(model = TESTING_SAMPLE_SCRIPT, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Why did skynet make you?\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\")\n",
    "\n",
    "# what happens when roles are swapped?\n",
    "response = ollama.chat(model = TESTING_SAMPLE_SCRIPT, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"I am the Terminator.\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\")\n",
    "\n",
    "# what about when the messages are slightly different?\n",
    "response = ollama.chat(model = TESTING_SAMPLE_SCRIPT, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Who are you Terminator?\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a4120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'models': [{'name': 'llama3.1:latest', 'model': 'llama3.1:latest', 'modified_at': '2024-10-04T12:05:19.2990124-07:00', 'size': 4661230766, 'digest': '42182419e9508c30c4b1fe55015f06b65f4ca4b9e28a744be55008d21998a093', 'details': {'parent_model': '', 'format': 'gguf', 'family': 'llama', 'families': ['llama'], 'parameter_size': '8.0B', 'quantization_level': 'Q4_0'}}]}\n"
     ]
    }
   ],
   "source": [
    "# Cleanup the tests\n",
    "ollama.delete(TESTING_LUIGI)\n",
    "ollama.delete(TESTING_SAMPLE_SCRIPT)\n",
    "print(ollama.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6ac2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          lineID characterID movieID characterName  \\\n",
      "0         L1045          u0      m0        BIANCA   \n",
      "1         L1044          u2      m0       CAMERON   \n",
      "2          L985          u0      m0        BIANCA   \n",
      "3          L984          u2      m0       CAMERON   \n",
      "4          L925          u0      m0        BIANCA   \n",
      "...         ...         ...     ...           ...   \n",
      "304708  L666371       u9030    m616      DURNFORD   \n",
      "304709  L666370       u9034    m616       VEREKER   \n",
      "304710  L666369       u9030    m616      DURNFORD   \n",
      "304711  L666257       u9030    m616      DURNFORD   \n",
      "304712  L666256       u9034    m616       VEREKER   \n",
      "\n",
      "                                                 lineText  \n",
      "0                                            They do not!  \n",
      "1                                             They do to!  \n",
      "2                                              I hope so.  \n",
      "3                                               She okay?  \n",
      "4                                               Let's go.  \n",
      "...                                                   ...  \n",
      "304708  Lord Chelmsford seems to want me to stay back ...  \n",
      "304709  I'm to take the Sikali with the main column to...  \n",
      "304710                           Your orders, Mr Vereker?  \n",
      "304711  Good ones, yes, Mr Vereker. Gentlemen who can ...  \n",
      "304712  Colonel Durnford... William Vereker. I hear yo...  \n",
      "\n",
      "[304713 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "# pull in the movie quotes data\n",
    "import pandas as pd\n",
    "\n",
    "# for the sep param: anything more than a char is treated as a reg ex\n",
    "AllMovieLines = pd.read_csv(\".\\\\data\\\\movie_lines-ANSI.txt\", sep = \" \\+\\+\\+\\$\\+\\+\\+ \", engine = \"python\")\n",
    "print(AllMovieLines.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "027b17c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model file creation: 2024-10-09 19:39:12.140884\n",
      "Index 0: 2024-10-09 19:39:12.141887\n",
      "FROM llama3.1\n",
      "\n",
      "\n",
      "MESSAGE assistant They do not!\n",
      "MESSAGE user They do to!\n",
      "MESSAGE assistant I hope so.\n",
      "MESSAGE user She okay?\n",
      "MESSAGE assistant Let's go.\n",
      "MESSAGE user Wow\n",
      "MESSAGE assistant Okay -- you're gonna need to learn how to lie.\n",
      "MESSAGE user No\n",
      "MESSAGE assistant I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
      "MESSAGE assistant Like my fear of wearing pastels?\n",
      "MESSAGE user The \"real you\".\n",
      "Index 25000: 2024-10-09 19:39:14.170467\n",
      "Index 50000: 2024-10-09 19:39:16.340989\n",
      "Index 75000: 2024-10-09 19:39:18.348442\n",
      "Index 100000: 2024-10-09 19:39:20.406337\n",
      "Index 125000: 2024-10-09 19:39:22.497981\n",
      "Index 150000: 2024-10-09 19:39:24.584153\n",
      "Index 175000: 2024-10-09 19:39:26.701058\n",
      "Index 200000: 2024-10-09 19:39:28.830691\n",
      "Index 225000: 2024-10-09 19:39:30.916415\n",
      "Index 250000: 2024-10-09 19:39:33.038498\n",
      "Index 275000: 2024-10-09 19:39:35.362893\n",
      "Index 300000: 2024-10-09 19:39:37.416667\n",
      "Finish model file creation: 2024-10-09 19:39:37.848056\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "ASSISTANT_CONST = True\n",
    "USER_CONST = False\n",
    "CHARACTER_ID_COL = \"characterID\"\n",
    "LINE_TEXT_COL = \"lineText\"\n",
    "\n",
    "MovieScriptFile = StringIO()\n",
    "MovieScriptFile.write(\"FROM llama3.1\\n\\n\")\n",
    "\n",
    "# looks like users are unique across the movies\n",
    "# map these ids to assistant (true) and user (false)\n",
    "ConstOfNextName = True\n",
    "DictOfUsers = {}    \n",
    "\n",
    "# let's try building up the model file by just ping\n",
    "# ponging the messages between user and assistant \n",
    "# as we encounter new characters and movies.\n",
    "print(\"Start model file creation: \" + str(datetime.now()))\n",
    "for CurIndex, CurRow in AllMovieLines.iterrows():\n",
    "    if CurIndex % 25000 == 0:\n",
    "        print(\"Index \" + str(CurIndex) + \": \" + str(datetime.now()))\n",
    "    \n",
    "    # looks like there are some blank ones and garbage, so just skip those\n",
    "    if CurRow[CHARACTER_ID_COL] is None or CurRow[CHARACTER_ID_COL] == \"\" \\\n",
    "    or CurRow[LINE_TEXT_COL] is None or CurRow[LINE_TEXT_COL] == \"\":\n",
    "        continue\n",
    "        \n",
    "    # add into the dict if necessary\n",
    "    if CurRow[CHARACTER_ID_COL] not in DictOfUsers:\n",
    "        DictOfUsers[CurRow[CHARACTER_ID_COL]] = ConstOfNextName\n",
    "        if ConstOfNextName == ASSISTANT_CONST:    # ping pong between the two\n",
    "            ConstOfNextName = USER_CONST\n",
    "        else:\n",
    "            ConstOfNextName = ASSISTANT_CONST\n",
    "    \n",
    "    # using StringIO to append strings took < 1 minute. Using string\n",
    "    # concat (x += y) took over 5 minutes, just for the first 100,000\n",
    "    # and was getting slower and slower\n",
    "    # Reference: https://realpython.com/python-string-concatenation/\n",
    "    if DictOfUsers[CurRow[CHARACTER_ID_COL]] == ASSISTANT_CONST:\n",
    "        MovieScriptFile.write(\"\\nMESSAGE assistant \" + CurRow[LINE_TEXT_COL])\n",
    "    else:\n",
    "        MovieScriptFile.write(\"\\nMESSAGE user \" + CurRow[LINE_TEXT_COL])\n",
    "    \n",
    "    if CurIndex == 10:    # print out the first part, just to ensure it looks ok\n",
    "        print(MovieScriptFile.getvalue())\n",
    "        \n",
    "print(\"Finish model file creation: \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcba0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model creation: 2024-10-09 19:40:24.296828\n",
      "Finish model creation: 2024-10-09 19:40:37.015223\n"
     ]
    }
   ],
   "source": [
    "# customize it \n",
    "MOVIE_SCRIPT_MODEL = \"MovieScript\"\n",
    "\n",
    "print(\"Start model creation: \" + str(datetime.now()))\n",
    "ollama.create(model = MOVIE_SCRIPT_MODEL, modelfile = MovieScriptFile.getvalue())\n",
    "print(\"Finish model creation: \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a5b269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "def MapUserIdToUserOrAssistant(BoolValue):\n",
    "    if BoolValue == ASSISTANT_CONST:\n",
    "        return \"assistant\"\n",
    "    else:\n",
    "        return \"user\"\n",
    "    \n",
    "# some tests from the first few lines\n",
    "# Raw:\n",
    "# L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
    "# L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
    "# Model file:\n",
    "# MESSAGE assistant They do not!\n",
    "# MESSAGE user They do to!\n",
    "print(MapUserIdToUserOrAssistant(DictOfUsers[\"u0\"]))\n",
    "print(MapUserIdToUserOrAssistant(DictOfUsers[\"u2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20224eb",
   "metadata": {},
   "source": [
    "## Conversation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83576580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "user\n",
      "Mrs Pulleine. A bit of a worry for us all, perhaps, but we'll have to make do without Mr Chard's presence just now.\n",
      "*** END OF RESPONSE ***\n",
      "\n",
      "Frau Blucher... and her husband the Baron.\n",
      "*** END OF RESPONSE ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw text from first random sample:\n",
    "#L496442 +++$+++ u2678 +++$+++ m174 +++$+++ MISS SUMMERS +++$+++ Are you Dr. Louis Judd?\n",
    "#L496440 +++$+++ u2669 +++$+++ m174 +++$+++ DR. JUDD +++$+++ Thank you.\n",
    "#L496439 +++$+++ u2678 +++$+++ m174 +++$+++ MISS SUMMERS +++$+++ Mr. Ward will see you in just a few minutes. Won't you wait, Dr. Judd?\n",
    "#L496478 +++$+++ u2671 +++$+++ m174 +++$+++ GREGORY +++$+++ What circumstances?\n",
    "#L496477 +++$+++ u2669 +++$+++ m174 +++$+++ DR. JUDD +++$+++ She's nervous, naturally, under the circumstances.\n",
    "#L496476 +++$+++ u2671 +++$+++ m174 +++$+++ GREGORY +++$+++ But tell me --\n",
    "#L496475 +++$+++ u2669 +++$+++ m174 +++$+++ DR. JUDD +++$+++ Oh, as beautiful as ever.\n",
    "#L496474 +++$+++ u2671 +++$+++ m174 +++$+++ GREGORY +++$+++ Tell me, how is Jacqueline?\n",
    "#L496473 +++$+++ u2669 +++$+++ m174 +++$+++ DR. JUDD +++$+++ For the time being, I imagine that must do.\n",
    "\n",
    "print(MapUserIdToUserOrAssistant(DictOfUsers[\"u2678\"]))\n",
    "print(MapUserIdToUserOrAssistant(DictOfUsers[\"u2669\"]))\n",
    "\n",
    "response = ollama.chat(model = MOVIE_SCRIPT_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"She's nervous, naturally, under the circumstances.\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\\n\")\n",
    "\n",
    "response = ollama.chat(model = MOVIE_SCRIPT_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is she worrying about?\",\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab70415",
   "metadata": {},
   "source": [
    "## Conversation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a399cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "assistant\n",
      "\n",
      "No. We're off in about an hour. You'll be with me on the 24th.\n",
      "*** END OF RESPONSE ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random raw text\n",
    "#L254476 +++$+++ u5623 +++$+++ m374 +++$+++ AL +++$+++ Ain't you gonna look back, Ma?--give the ol' place a last look?\n",
    "#L255177 +++$+++ u5625 +++$+++ m374 +++$+++ BLONDE GIRL +++$+++ Now I can't breathe.\n",
    "#L255176 +++$+++ u5623 +++$+++ m374 +++$+++ AL +++$+++ That comes from not holdin' you tight *enough.*\n",
    "#L255175 +++$+++ u5625 +++$+++ m374 +++$+++ BLONDE GIRL +++$+++ You're *ticklin' me!*\n",
    "#L255174 +++$+++ u5623 +++$+++ m374 +++$+++ AL +++$+++ Why, I ain't hardly touchin' you!\n",
    "#L255173 +++$+++ u5625 +++$+++ m374 +++$+++ BLONDE GIRL +++$+++ Don't hold me so tight.\n",
    "print(MapUserIdToUserOrAssistant(DictOfUsers[\"u5623\"]))\n",
    "print(MapUserIdToUserOrAssistant(DictOfUsers[\"u5625\"]))\n",
    "print()\n",
    "\n",
    "response = ollama.chat(model = MOVIE_SCRIPT_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Ain't you gonna look back, Ma?--give the ol' place a last look?\",    # cut and paste of the first line\n",
    "\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e48f1350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're going to follow the main column, Sir. The Twenty-Fourth Regiment of Foot will be taking post at Isandlwana.\n",
      "*** END OF RESPONSE ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model = MOVIE_SCRIPT_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Where are we going in an hour? What is on the 24th?\",  \n",
    "\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7cafca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a bloody big one, sir... And we're being sent to the wrong place!\n",
      "*** END OF RESPONSE ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model = MOVIE_SCRIPT_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Who is the main column going to attack? Is Isandlwana and important position?\", \n",
    "\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84bd1736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rorke's Drift, sir. A small post on the border. We could send a message to Pietermaritzburg, and they would likely reinforce us. But it would mean abandoning the rest of the troops...\n",
      "*** END OF RESPONSE ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model = MOVIE_SCRIPT_MODEL, messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Where should we go instead? How can we save ourselves?\", \n",
    "\n",
    "  },\n",
    "])\n",
    "print(response[\"message\"][\"content\"])\n",
    "print(\"*** END OF RESPONSE ***\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dce8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
