{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 74,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013513513513513514,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 3.5748,
      "step": 1
    },
    {
      "epoch": 0.02702702702702703,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 5.0893,
      "step": 2
    },
    {
      "epoch": 0.04054054054054054,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 4.2369,
      "step": 3
    },
    {
      "epoch": 0.05405405405405406,
      "grad_norm": NaN,
      "learning_rate": 0.0,
      "loss": 4.2663,
      "step": 4
    },
    {
      "epoch": 0.06756756756756757,
      "grad_norm": 1.435181736946106,
      "learning_rate": 5e-06,
      "loss": 4.6418,
      "step": 5
    },
    {
      "epoch": 0.08108108108108109,
      "grad_norm": NaN,
      "learning_rate": 5e-06,
      "loss": 4.0142,
      "step": 6
    },
    {
      "epoch": 0.0945945945945946,
      "grad_norm": 1.4436676502227783,
      "learning_rate": 1e-05,
      "loss": 4.2519,
      "step": 7
    },
    {
      "epoch": 0.10810810810810811,
      "grad_norm": 1.475297451019287,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 3.9525,
      "step": 8
    },
    {
      "epoch": 0.12162162162162163,
      "grad_norm": 1.7963213920593262,
      "learning_rate": 2e-05,
      "loss": 4.6194,
      "step": 9
    },
    {
      "epoch": 0.13513513513513514,
      "grad_norm": 1.4146817922592163,
      "learning_rate": 1.9989930665413148e-05,
      "loss": 4.3707,
      "step": 10
    },
    {
      "epoch": 0.14864864864864866,
      "grad_norm": 1.2773265838623047,
      "learning_rate": 1.9959742939952393e-05,
      "loss": 4.0281,
      "step": 11
    },
    {
      "epoch": 0.16216216216216217,
      "grad_norm": 1.391348958015442,
      "learning_rate": 1.990949761767935e-05,
      "loss": 4.0379,
      "step": 12
    },
    {
      "epoch": 0.17567567567567569,
      "grad_norm": 1.60454261302948,
      "learning_rate": 1.98392958859863e-05,
      "loss": 4.3417,
      "step": 13
    },
    {
      "epoch": 0.1891891891891892,
      "grad_norm": 1.7470682859420776,
      "learning_rate": 1.9749279121818235e-05,
      "loss": 4.6451,
      "step": 14
    },
    {
      "epoch": 0.20270270270270271,
      "grad_norm": 1.7162431478500366,
      "learning_rate": 1.9639628606958535e-05,
      "loss": 4.1923,
      "step": 15
    },
    {
      "epoch": 0.21621621621621623,
      "grad_norm": 1.7930734157562256,
      "learning_rate": 1.9510565162951538e-05,
      "loss": 4.4469,
      "step": 16
    },
    {
      "epoch": 0.22972972972972974,
      "grad_norm": 1.3952652215957642,
      "learning_rate": 1.9362348706397374e-05,
      "loss": 3.5344,
      "step": 17
    },
    {
      "epoch": 0.24324324324324326,
      "grad_norm": 1.9047831296920776,
      "learning_rate": 1.919527772551451e-05,
      "loss": 4.5117,
      "step": 18
    },
    {
      "epoch": 0.25675675675675674,
      "grad_norm": 1.748903512954712,
      "learning_rate": 1.900968867902419e-05,
      "loss": 4.117,
      "step": 19
    },
    {
      "epoch": 0.2702702702702703,
      "grad_norm": 1.8254340887069702,
      "learning_rate": 1.880595531856738e-05,
      "loss": 3.9778,
      "step": 20
    },
    {
      "epoch": 0.28378378378378377,
      "grad_norm": 2.1246466636657715,
      "learning_rate": 1.8584487936018663e-05,
      "loss": 4.4624,
      "step": 21
    },
    {
      "epoch": 0.2972972972972973,
      "grad_norm": 1.429299235343933,
      "learning_rate": 1.834573253721303e-05,
      "loss": 3.3695,
      "step": 22
    },
    {
      "epoch": 0.3108108108108108,
      "grad_norm": 1.7220278978347778,
      "learning_rate": 1.8090169943749477e-05,
      "loss": 3.5194,
      "step": 23
    },
    {
      "epoch": 0.32432432432432434,
      "grad_norm": 1.9180272817611694,
      "learning_rate": 1.78183148246803e-05,
      "loss": 3.8136,
      "step": 24
    },
    {
      "epoch": 0.33783783783783783,
      "grad_norm": 1.818749189376831,
      "learning_rate": 1.7530714660036112e-05,
      "loss": 3.6278,
      "step": 25
    },
    {
      "epoch": 0.35135135135135137,
      "grad_norm": 1.9443881511688232,
      "learning_rate": 1.7227948638273918e-05,
      "loss": 3.6481,
      "step": 26
    },
    {
      "epoch": 0.36486486486486486,
      "grad_norm": 2.121446132659912,
      "learning_rate": 1.691062648986865e-05,
      "loss": 3.8167,
      "step": 27
    },
    {
      "epoch": 0.3783783783783784,
      "grad_norm": 1.8181613683700562,
      "learning_rate": 1.657938725939713e-05,
      "loss": 3.1996,
      "step": 28
    },
    {
      "epoch": 0.3918918918918919,
      "grad_norm": 1.3707510232925415,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 3.2627,
      "step": 29
    },
    {
      "epoch": 0.40540540540540543,
      "grad_norm": 2.4721271991729736,
      "learning_rate": 1.5877852522924733e-05,
      "loss": 3.7858,
      "step": 30
    },
    {
      "epoch": 0.4189189189189189,
      "grad_norm": 2.0488030910491943,
      "learning_rate": 1.5508969814521026e-05,
      "loss": 3.3547,
      "step": 31
    },
    {
      "epoch": 0.43243243243243246,
      "grad_norm": 1.4683053493499756,
      "learning_rate": 1.5128992774059063e-05,
      "loss": 3.1377,
      "step": 32
    },
    {
      "epoch": 0.44594594594594594,
      "grad_norm": 1.7102932929992676,
      "learning_rate": 1.4738686624729987e-05,
      "loss": 3.1879,
      "step": 33
    },
    {
      "epoch": 0.4594594594594595,
      "grad_norm": 1.881758451461792,
      "learning_rate": 1.4338837391175582e-05,
      "loss": 3.367,
      "step": 34
    },
    {
      "epoch": 0.47297297297297297,
      "grad_norm": 1.7505069971084595,
      "learning_rate": 1.3930250316539237e-05,
      "loss": 2.9829,
      "step": 35
    },
    {
      "epoch": 0.4864864864864865,
      "grad_norm": 2.3413758277893066,
      "learning_rate": 1.3513748240813429e-05,
      "loss": 3.6692,
      "step": 36
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.9367725849151611,
      "learning_rate": 1.3090169943749475e-05,
      "loss": 3.0499,
      "step": 37
    },
    {
      "epoch": 0.5135135135135135,
      "grad_norm": 1.3000142574310303,
      "learning_rate": 1.2660368455666752e-05,
      "loss": 3.1808,
      "step": 38
    },
    {
      "epoch": 0.527027027027027,
      "grad_norm": 2.238621950149536,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 3.6605,
      "step": 39
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 1.487630844116211,
      "learning_rate": 1.1785568947986368e-05,
      "loss": 3.0669,
      "step": 40
    },
    {
      "epoch": 0.5540540540540541,
      "grad_norm": 1.649430274963379,
      "learning_rate": 1.1342332658176556e-05,
      "loss": 3.6197,
      "step": 41
    },
    {
      "epoch": 0.5675675675675675,
      "grad_norm": 1.6846239566802979,
      "learning_rate": 1.0896393089034336e-05,
      "loss": 3.1598,
      "step": 42
    },
    {
      "epoch": 0.581081081081081,
      "grad_norm": 1.196217656135559,
      "learning_rate": 1.044864830350515e-05,
      "loss": 3.163,
      "step": 43
    },
    {
      "epoch": 0.5945945945945946,
      "grad_norm": 1.257106065750122,
      "learning_rate": 1e-05,
      "loss": 3.0497,
      "step": 44
    },
    {
      "epoch": 0.6081081081081081,
      "grad_norm": 1.1109914779663086,
      "learning_rate": 9.551351696494854e-06,
      "loss": 2.8251,
      "step": 45
    },
    {
      "epoch": 0.6216216216216216,
      "grad_norm": 1.3542242050170898,
      "learning_rate": 9.103606910965666e-06,
      "loss": 3.1964,
      "step": 46
    },
    {
      "epoch": 0.6351351351351351,
      "grad_norm": 1.1424102783203125,
      "learning_rate": 8.657667341823449e-06,
      "loss": 2.9634,
      "step": 47
    },
    {
      "epoch": 0.6486486486486487,
      "grad_norm": 1.3313719034194946,
      "learning_rate": 8.214431052013636e-06,
      "loss": 2.6582,
      "step": 48
    },
    {
      "epoch": 0.6621621621621622,
      "grad_norm": 1.1022610664367676,
      "learning_rate": 7.774790660436857e-06,
      "loss": 2.8139,
      "step": 49
    },
    {
      "epoch": 0.6756756756756757,
      "grad_norm": 1.3178200721740723,
      "learning_rate": 7.33963154433325e-06,
      "loss": 3.4933,
      "step": 50
    },
    {
      "epoch": 0.6891891891891891,
      "grad_norm": 1.1161611080169678,
      "learning_rate": 6.909830056250527e-06,
      "loss": 2.748,
      "step": 51
    },
    {
      "epoch": 0.7027027027027027,
      "grad_norm": 1.092895746231079,
      "learning_rate": 6.486251759186573e-06,
      "loss": 2.8119,
      "step": 52
    },
    {
      "epoch": 0.7162162162162162,
      "grad_norm": 1.3077768087387085,
      "learning_rate": 6.069749683460765e-06,
      "loss": 2.849,
      "step": 53
    },
    {
      "epoch": 0.7297297297297297,
      "grad_norm": 1.3406219482421875,
      "learning_rate": 5.66116260882442e-06,
      "loss": 3.1874,
      "step": 54
    },
    {
      "epoch": 0.7432432432432432,
      "grad_norm": 1.3176409006118774,
      "learning_rate": 5.2613133752700145e-06,
      "loss": 3.2723,
      "step": 55
    },
    {
      "epoch": 0.7567567567567568,
      "grad_norm": 1.4749008417129517,
      "learning_rate": 4.87100722594094e-06,
      "loss": 3.0855,
      "step": 56
    },
    {
      "epoch": 0.7702702702702703,
      "grad_norm": 1.659837245941162,
      "learning_rate": 4.491030185478976e-06,
      "loss": 3.4772,
      "step": 57
    },
    {
      "epoch": 0.7837837837837838,
      "grad_norm": 1.241044044494629,
      "learning_rate": 4.12214747707527e-06,
      "loss": 3.1691,
      "step": 58
    },
    {
      "epoch": 0.7972972972972973,
      "grad_norm": 1.6158474683761597,
      "learning_rate": 3.7651019814126656e-06,
      "loss": 3.4569,
      "step": 59
    },
    {
      "epoch": 0.8108108108108109,
      "grad_norm": 1.0538233518600464,
      "learning_rate": 3.4206127406028744e-06,
      "loss": 2.9462,
      "step": 60
    },
    {
      "epoch": 0.8243243243243243,
      "grad_norm": 1.1804338693618774,
      "learning_rate": 3.089373510131354e-06,
      "loss": 3.0237,
      "step": 61
    },
    {
      "epoch": 0.8378378378378378,
      "grad_norm": 1.3688762187957764,
      "learning_rate": 2.7720513617260857e-06,
      "loss": 3.3856,
      "step": 62
    },
    {
      "epoch": 0.8513513513513513,
      "grad_norm": 1.0069587230682373,
      "learning_rate": 2.469285339963892e-06,
      "loss": 2.6612,
      "step": 63
    },
    {
      "epoch": 0.8648648648648649,
      "grad_norm": 1.078506350517273,
      "learning_rate": 2.1816851753197023e-06,
      "loss": 3.0665,
      "step": 64
    },
    {
      "epoch": 0.8783783783783784,
      "grad_norm": 1.0839626789093018,
      "learning_rate": 1.9098300562505266e-06,
      "loss": 3.0173,
      "step": 65
    },
    {
      "epoch": 0.8918918918918919,
      "grad_norm": 1.1105823516845703,
      "learning_rate": 1.6542674627869738e-06,
      "loss": 3.1753,
      "step": 66
    },
    {
      "epoch": 0.9054054054054054,
      "grad_norm": 0.996198832988739,
      "learning_rate": 1.4155120639813392e-06,
      "loss": 3.0722,
      "step": 67
    },
    {
      "epoch": 0.918918918918919,
      "grad_norm": 1.2098112106323242,
      "learning_rate": 1.19404468143262e-06,
      "loss": 3.1628,
      "step": 68
    },
    {
      "epoch": 0.9324324324324325,
      "grad_norm": 1.3499552011489868,
      "learning_rate": 9.903113209758098e-07,
      "loss": 3.7117,
      "step": 69
    },
    {
      "epoch": 0.9459459459459459,
      "grad_norm": 1.1609231233596802,
      "learning_rate": 8.047222744854943e-07,
      "loss": 3.13,
      "step": 70
    },
    {
      "epoch": 0.9594594594594594,
      "grad_norm": 1.4739539623260498,
      "learning_rate": 6.37651293602628e-07,
      "loss": 3.3649,
      "step": 71
    },
    {
      "epoch": 0.972972972972973,
      "grad_norm": 1.0678105354309082,
      "learning_rate": 4.894348370484648e-07,
      "loss": 3.0244,
      "step": 72
    },
    {
      "epoch": 0.9864864864864865,
      "grad_norm": 1.2883371114730835,
      "learning_rate": 3.603713930414676e-07,
      "loss": 3.2285,
      "step": 73
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.0816127061843872,
      "learning_rate": 2.507208781817638e-07,
      "loss": 3.2178,
      "step": 74
    }
  ],
  "logging_steps": 1,
  "max_steps": 74,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1598518468804608.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
