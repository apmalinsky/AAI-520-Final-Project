{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data readers\n",
    "from ipynb.fs.defs.data_preparation import read_movie_metadata, read_character_metadata, read_line_data, read_conversations_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = read_movie_metadata()\n",
    "character_df = read_character_metadata(movie_df)\n",
    "line_df = read_line_data(movie_df, character_df)\n",
    "conversation_df = read_conversations_data(movie_df, character_df, line_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 617\n",
      "Number of characters: 9035\n",
      "Number of lines: 304713\n",
      "Number of conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of movies: {len(movie_df)}\")\n",
    "print(f\"Number of characters: {len(character_df)}\")\n",
    "print(f\"Number of lines: {len(line_df)}\")\n",
    "print(f\"Number of conversations: {len(conversation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = conversation_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversation(conversation_data, line_df):\n",
    "    # Turn conversation into chat format of one input and one response\n",
    "    # For each pair of lines, add start of sentence token, end of sentence token, and bot response\n",
    "\n",
    "    chats = []\n",
    "    for data in conversation_data:\n",
    "        # If it is not even, drop the last line as it is missing the bot response\n",
    "        if len(data) % 2 == 1:\n",
    "            data = data[:-1]\n",
    "\n",
    "        for idx, line in enumerate(data):\n",
    "            line_text = line_df[line_df[\"id\"] == line][\"line\"].values[0]\n",
    "\n",
    "            if idx % 2 == 0:\n",
    "                chat = \" \".join([\"<SOS>\", line_text])\n",
    "            else:\n",
    "                chat = \" \".join([chat, \"<BOT>\", line_text, \"<EOS>\"])\n",
    "                chats.append(chat)\n",
    "\n",
    "    return chats\n",
    "\n",
    "\n",
    "chats = convert_to_conversation(conversation_df[\"lines\"].values, line_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Stuff\\School\\MSAAI\\AAI 520\\final_project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add special tokens for start/end of sentence, padding, and bot response\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\", \"bos_token\": \"<SOS>\", \"eos_token\": \"<EOS>\"})\n",
    "tokenizer.add_tokens([\"<BOT>\"])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "d:\\My Stuff\\School\\MSAAI\\AAI 520\\final_project\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Hi how are you?  <BOT>  <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(**tokenizer(\"<SOS> Hi how are you? <BOT> \", return_tensors=\"pt\"))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, chats, tokenizer):\n",
    "        self.chats = chats\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 128\n",
    "\n",
    "        self.encoded_data = self.tokenizer(self.chats, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        self.input_id = self.encoded_data[\"input_ids\"]\n",
    "        self.attention_mask = self.encoded_data[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_id[idx], self.attention_mask[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def train_model(chat_data, model, optimizer, device, epochs=10, save_every=5):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch = 0\n",
    "        for input_id, attention_mask in chat_data:\n",
    "            batch += 1\n",
    "            # print(f\"Batch: {batch}\")\n",
    "            input_id = input_id.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            output = model(input_id, attention_mask=attention_mask, labels=input_id)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Print example chat\n",
    "        # example = tokenizer(\"<SOS> Hi I am Ethan, how are you? <BOT>\", return_tensors=\"pt\")\n",
    "        # input_id = example[\"input_ids\"].to(device)\n",
    "        # attention_mask = example[\"attention_mask\"].to(device)\n",
    "        # print(tokenizer.decode(model.generate(input_id, attention_mask=attention_mask)[0]))\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save(model.state_dict(), Path(f\"models/gpt2/checkpoints/model_{epoch}.pt\"))\n",
    "\n",
    "    torch.save(model.state_dict(), Path(f\"models/gpt2/final/model_final.pt\"))\n",
    "\n",
    "def inference(model, tokenizer, device):\n",
    "    model.eval()\n",
    "    chat = input(\"User: \")\n",
    "    while chat != \"quit\" and chat != \"q\":\n",
    "        chat = tokenizer(\" \".join([\"<SOS>\", chat, \"<BOT>\"]), return_tensors=\"pt\")\n",
    "\n",
    "        input_id = chat[\"input_ids\"].to(device)\n",
    "        attention_mask = chat[\"attention_mask\"].to(device)\n",
    "        output = model.generate(input_id, attention_mask=attention_mask, max_length=128)\n",
    "        print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "        chat = input(\"User: \")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "chat_data = DataLoader(ChatData(chats[0:8192], tokenizer), batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\My Stuff\\School\\MSAAI\\AAI 520\\final_project\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 100/100 [4:33:17<00:00, 163.97s/it] \n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "train_model(chat_data, model, optimizer, device, epochs=100, save_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hi how are you?  <BOT>  Been better.\n",
      "  How are you?\n",
      "  How are you?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You said you've been better, whats wrong  <BOT>  You won't be back in town two weeks from now.\n",
      "  Where are you going?\n",
      "  It's just a long drive.\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Im goin to Alaska  <BOT>  Make yourself at home.  I'll call you a cab.\n",
      "  Let's go.\n",
      "  How are we going to make it?\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pee pee poo poo  <BOT>  Poppa Joe said there was only two. In and out.  Boy, you guys sure did a good job. You're good, huh? Cool masks. Where'd you get them?\n",
      "  <BOT>  Let's do him right here.\n",
      "  We gotta go.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pee Pee Poo Poo  <BOT>  I can't believe you've sunk so low.\n",
      "  <BOT>  I'm sorry, Gary, but I've got to have a drink.\n",
      "  We're just about to begin the process.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "inference(model, tokenizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
