{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in movie titles metadata.txt\n",
    "def read_movie_metadata() -> pd.DataFrame:\n",
    "    movie_metadata = {}\n",
    "    with open(\"data/movie_titles_metadata.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Strip off \"m\" from id and convert to int\n",
    "            line[0] = int(line[0][1:])\n",
    "\n",
    "            # Lowercase title\n",
    "            line[1] = line[1].lower()\n",
    "\n",
    "            # Check if the year has an \"I\" at the end, if so, remove it then convert to int\n",
    "            if line[2][-1] ==\"I\":\n",
    "                line[2] = line[2][:-2]\n",
    "\n",
    "            try:\n",
    "                line[2] = int(line[2])\n",
    "            except:\n",
    "                print(f\"Movie {line[0]}/'{line[1]}' - Invalid year: {line[2]}\")\n",
    "                continue\n",
    "\n",
    "            # Convert IMDB rating to float\n",
    "            line[3] = float(line[3])\n",
    "\n",
    "            # Convert IMDB votes to int\n",
    "            line[4] = int(line[4])\n",
    "\n",
    "            # Strip off spaces, [], \\n, and '' from genres\n",
    "            line[5] = line[5].strip(\"\\n[]\").replace(\"'\", \"\").replace(\" \", \"\").split(\",\")\n",
    "\n",
    "            # Fields: movie ID, movie title, movie year, IMDB rating, number of IMDB votes, genres in the format ['genre1','genre2',�,'genreN']\n",
    "            movie_metadata |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"title\": line[1],\n",
    "                    \"year\": line[2],\n",
    "                    \"IMDB_rating\": line[3],\n",
    "                    \"IMDB_votes\": line[4],\n",
    "                    \"genres\": line[5],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(movie_metadata, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_character_metadata(movie_metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Read in movie characters metadata.txt\n",
    "    character_metadata = {}\n",
    "    with open(\"data/movie_characters_metadata.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Strip off \"c\" from id and convert to int\n",
    "            line[0] = int(line[0][1:])\n",
    "\n",
    "            # Lowercase character name\n",
    "            line[1] = line[1].lower()\n",
    "\n",
    "            # Strip off m from movie id and convert to int, then ensure movie id is valid\n",
    "            line[2] = int(line[2][1:])\n",
    "            if line[2] not in movie_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Character - {line[0]}/'{line[1]}': Movie ID: {line[2]} does not exist in movie_titles_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Lowercase movie title\n",
    "            line[3] = line[3].lower()\n",
    "\n",
    "            # Convert gender to boolean (0 = male, 1 = female, ? = nan)\n",
    "            if line[4] == \"m\":\n",
    "                line[4] = False\n",
    "            elif line[4] == \"f\":\n",
    "                line[4] = True\n",
    "            elif line[4] == \"?\":\n",
    "                line[4] = np.nan\n",
    "\n",
    "            # Convert position to int and remove \"\\n\"\n",
    "            line[5] = line[5][:-1]\n",
    "            if line[5][-1] == \"?\":\n",
    "                line[5] = np.nan\n",
    "            else:\n",
    "                line[5] = int(line[5])\n",
    "\n",
    "            # Fields: character ID, character name, movie id, movie title, gender (\"?\" for unlabeled cases), position in credits (\"?\" for unlabeled cases)\n",
    "            character_metadata |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"name\": line[1],\n",
    "                    \"movie_id\": line[2],\n",
    "                    \"movie_title\": line[3],\n",
    "                    \"gender\": line[4],\n",
    "                    \"position\": line[5],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(character_metadata, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_data(\n",
    "    movie_metadata_df: pd.DataFrame, character_metadata_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Read in movie lines .txt\n",
    "    line_data = {}\n",
    "    with open(\"data/movie_lines.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Strip off \"L\" from line id and convert to int\n",
    "            line[0] = int(line[0][1:])\n",
    "\n",
    "            # Strip off \"c\" from character id and convert to int, then ensure character id is valid\n",
    "            line[1] = int(line[1][1:])\n",
    "            if line[1] not in character_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Line - {line[0]}: Character ID: {line[1]} does not exist in movie_characters_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Strip of \"m\" from movie id and convert to int, then ensure movie id is valid\n",
    "            line[2] = int(line[2][1:])\n",
    "            if line[2] not in movie_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Line - {line[0]}: Movie ID: {line[2]} does not exist in movie_titles_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Lowercase character name\n",
    "            line[3] = line[3].lower()\n",
    "\n",
    "            # Fields: line ID, character ID, movie id, character name, text\n",
    "            line_data |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"character_id\": line[1],\n",
    "                    \"movie_id\": line[2],\n",
    "                    \"character_name\": line[3],\n",
    "                    \"line\": line[4],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(line_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversations_data(\n",
    "    movie_metadata_df: pd.DataFrame,\n",
    "    character_metadata_df: pd.DataFrame,\n",
    "    line_data_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # Read in movie lines .txt\n",
    "    conversation_data = {}\n",
    "    with open(\"data/movie_conversations.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Create a conversation index\n",
    "            line.insert(0, idx)\n",
    "\n",
    "            # Strip off \"c\" from both character id's and convert to int, then ensure character id is valid for both characters\n",
    "            line[1] = int(line[1][1:])\n",
    "            line[2] = int(line[2][1:])\n",
    "            if line[1] not in character_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Conversation - {line[0]}: Character ID: {line[1]} does not exist in movie_characters_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            if line[2] not in character_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Conversation - {line[0]}: Character ID: {line[2]} does not exist in movie_characters_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Strip off \"m\" from movie index and convert to int, then ensure movie id is valid\n",
    "            line[3] = int(line[3][1:])\n",
    "            if line[3] not in movie_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Conversation - {line[0]}: Movie ID: {line[3]} does not exist in movie_titles_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Strip off spaces, L, [], \\n, and '' from lines, then ensure each line id is valid\n",
    "            line[4] = (\n",
    "                line[4].strip(\"\\n[]\").replace(\"'\", \"\").replace(\" \", \"\").replace(\"L\", \"\").split(\",\")\n",
    "            )\n",
    "            line[4] = [int(l) for l in line[4]]\n",
    "\n",
    "            invalid = False\n",
    "            for l in line[4]:\n",
    "                if l not in line_data_df[\"id\"].values:\n",
    "                    invalid = True\n",
    "\n",
    "            if invalid:\n",
    "                print(\n",
    "                        f\"Conversation - {line[0]}: A lineID it references does not exist in movie_lines.txt or has been removed.\"\n",
    "                    )\n",
    "\n",
    "            # Fields: first speaker character ID, second speaker character ID, movie ID, list of lines in chronological order ['lineID1', 'lineID2', �, 'lineIDN']\n",
    "            conversation_data |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"character ID 1\": line[1],\n",
    "                    \"character ID 2\": line[2],\n",
    "                    \"movie ID\": line[3],\n",
    "                    \"lines\": line[4],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(conversation_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 617\n",
      "Number of characters: 9035\n",
      "Number of lines: 304713\n",
      "Number of conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "# Final report on data\n",
    "movie_metadata_df = read_movie_metadata()\n",
    "character_metadata_df = read_character_metadata(movie_metadata_df)\n",
    "line_data_df = read_line_data(movie_metadata_df, character_metadata_df)\n",
    "conversation_data_df = read_conversations_data(movie_metadata_df, character_metadata_df, line_data_df)\n",
    "\n",
    "print(f\"Number of movies: {len(movie_metadata_df)}\")\n",
    "print(f\"Number of characters: {len(character_metadata_df)}\")\n",
    "print(f\"Number of lines: {len(line_data_df)}\")\n",
    "print(f\"Number of conversations: {len(conversation_data_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_metadata_df\n",
    "character_df = character_metadata_df\n",
    "line_df = line_data_df\n",
    "conversation_df = conversation_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 617\n",
      "Number of characters: 9035\n",
      "Number of lines: 304713\n",
      "Number of conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of movies: {len(movie_df)}\")\n",
    "print(f\"Number of characters: {len(character_df)}\")\n",
    "print(f\"Number of lines: {len(line_df)}\")\n",
    "print(f\"Number of conversations: {len(conversation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = conversation_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversation(conversation_data, line_df):\n",
    "    # Turn conversation into chat format of one input and one response\n",
    "    # For each pair of lines, add start of sentence token, end of sentence token, and bot response\n",
    "\n",
    "    chats = []\n",
    "    for data in conversation_data:\n",
    "        # If it is not even, drop the last line as it is missing the bot response\n",
    "        if len(data) % 2 == 1:\n",
    "            data = data[:-1]\n",
    "\n",
    "        for idx, line in enumerate(data):\n",
    "            line_text = line_df[line_df[\"id\"] == line][\"line\"].values[0]\n",
    "\n",
    "            if idx % 2 == 0:\n",
    "                chat = \" \".join([\"<SOS>\", line_text])\n",
    "            else:\n",
    "                chat = \" \".join([chat, \"<BOT>\", line_text, \"<EOS>\"])\n",
    "                chats.append(chat)\n",
    "\n",
    "    return chats\n",
    "\n",
    "\n",
    "chats = convert_to_conversation(conversation_df[\"lines\"].values, line_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add special tokens for start/end of sentence, padding, and bot response\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\", \"bos_token\": \"<SOS>\", \"eos_token\": \"<EOS>\"})\n",
    "tokenizer.add_tokens([\"<BOT>\"])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Hi how are you?  <BOT>  <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(**tokenizer(\"<SOS> Hi how are you? <BOT> \", return_tensors=\"pt\"))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, chats, tokenizer):\n",
    "        self.chats = chats\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 128\n",
    "\n",
    "        self.encoded_data = self.tokenizer(self.chats, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        self.input_id = self.encoded_data[\"input_ids\"]\n",
    "        self.attention_mask = self.encoded_data[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_id[idx], self.attention_mask[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50261, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def train_model(chat_data, model, optimizer, device, epochs=10, save_every=5):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch = 0\n",
    "        for input_id, attention_mask in chat_data:\n",
    "            batch += 1\n",
    "            # print(f\"Batch: {batch}\")\n",
    "            input_id = input_id.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            output = model(input_id, attention_mask=attention_mask, labels=input_id)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Print example chat\n",
    "        # example = tokenizer(\"<SOS> Hi I am Ethan, how are you? <BOT>\", return_tensors=\"pt\")\n",
    "        # input_id = example[\"input_ids\"].to(device)\n",
    "        # attention_mask = example[\"attention_mask\"].to(device)\n",
    "        # print(tokenizer.decode(model.generate(input_id, attention_mask=attention_mask)[0]))\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save(model.state_dict(), Path(f\"models/gpt2/checkpoints/model_{epoch}.pt\"))\n",
    "\n",
    "    torch.save(model.state_dict(), Path(f\"models/gpt2/final/model_final.pt\"))\n",
    "\n",
    "def truncate_history(history, tokenizer, max_length=128):\n",
    "    # Tokenizing history\n",
    "    encoded_history = tokenizer.encode(history, return_tensors=\"pt\")[0]\n",
    "\n",
    "    # If history exceeds max_length, truncate\n",
    "    if len(encoded_history) >= max_length:\n",
    "        truncated_history = encoded_history[-max_length:]\n",
    "    else:\n",
    "        truncated_history = encoded_history\n",
    "\n",
    "    truncated_text = tokenizer.decode(truncated_history, skip_special_tokens=False)\n",
    "\n",
    "    return truncated_text.strip()\n",
    "\n",
    "def generate_context(chat_history, max_entries=5):\n",
    "    # Generating context based off of history\n",
    "    context = \" \".join(list(chat_history)[-max_entries*2:])\n",
    "    return context.strip()\n",
    "\n",
    "def remove_repeated_lines(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    unique_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip() not in unique_lines:\n",
    "            unique_lines.append(line.strip())\n",
    "    return \" \".join(unique_lines)\n",
    "\n",
    "def inference(model, tokenizer, device):\n",
    "    model.eval()\n",
    "    chat_history = deque(maxlen=5)\n",
    "    chat = input(\"User: \")\n",
    "    \n",
    "    while chat.lower() != \"quit\" and chat != \"q\":\n",
    "        chat_history.append(f\"{chat}\")   # Adding user input to history\n",
    "        \n",
    "        context = generate_context(chat_history) # Generating context\n",
    "        truncated_history = truncate_history(context, tokenizer, max_length=128) # Truncating user history so it doesn't exceed max_length of 128 in longer convo\n",
    "        \n",
    "        chat_encoded = tokenizer(truncated_history, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "        input_id = chat_encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = chat_encoded[\"attention_mask\"].to(device)\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_id, \n",
    "            attention_mask=attention_mask, \n",
    "            max_new_tokens=30,\n",
    "            do_sample=True, # Set to true for use with temp, top-k, and top-p\n",
    "            temperature=0.5, # Lowering temperature for more coherent responses.\n",
    "            top_k=40, # Adding top-k sampling to limit to top 40 likely next tokens.\n",
    "            top_p=0.8, # Adding nucleus sampling as well.\n",
    "            repetition_penalty=1.4, # Penalizing repeated sequences\n",
    "            pad_token_id=tokenizer.eos_token_id # Handling end of sequence properly\n",
    "        )   \n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "        response = remove_repeated_lines(response)\n",
    "        response = response.replace(\"<BOT>\", \"\").replace(\"<SOS>\", \"\").strip()\n",
    "\n",
    "        # Preventing redundancy\n",
    "        if response and (len(chat_history) < 2 or not response.endswith(chat_history[-2])):\n",
    "            print(f\"Bot: {response}\")\n",
    "            chat_history.append(f\"<BOT> {response}\")\n",
    "        else:\n",
    "            print(\"Bot: I didn't catch that. Can you ask something else?\")\n",
    "\n",
    "        chat = input(\"User: \")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "chat_data = DataLoader(ChatData(chats[0:8192], tokenizer), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]C:\\Users\\seanp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [04:49<00:00, 57.92s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "train_model(chat_data, model, optimizer, device, epochs=5, save_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  How are you today?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: How are you today?   I'm in a hurry. What's the matter, Mrs.? We're all busy here at home now...I'll see ya later\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  Why are you in such a hurry?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: How are you today?      How are you today?   I'm in a hurry. What's the matter, Mrs.? We're all busy here at home now...I'll see ya later Why are you in such a hurry? The Doctor says he wants to talk about something else: how did your mother get so attached and what have we learned from her that she can't\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  What did the Doctor say?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: today?   I'm in a hurry. What's the matter, Mrs.? We're all busy here at home now...I'll see ya later Why are you in such a hurry?      How are you today?      How are you today?   I'm in a hurry. What's the matter, Mrs.? We're all busy here at home now...I'll see ya later Why are you in such a hurry? The Doctor says he wants to talk about something else: how did your mother get so attached and what have we learned from her that she can't What did the Doctor say is this - no one will ever know! \" You've got my number on it too Mr Dickson?\" \" Yes sir.\" He\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  q\n"
     ]
    }
   ],
   "source": [
    "inference(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What did you do today?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
