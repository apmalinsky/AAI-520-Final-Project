{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in movie titles metadata.txt\n",
    "def read_movie_metadata() -> pd.DataFrame:\n",
    "    movie_metadata = {}\n",
    "    with open(\"data/movie_titles_metadata.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Strip off \"m\" from id and convert to int\n",
    "            line[0] = int(line[0][1:])\n",
    "\n",
    "            # Lowercase title\n",
    "            line[1] = line[1].lower()\n",
    "\n",
    "            # Check if the year has an \"I\" at the end, if so, remove it then convert to int\n",
    "            if line[2][-1] ==\"I\":\n",
    "                line[2] = line[2][:-2]\n",
    "\n",
    "            try:\n",
    "                line[2] = int(line[2])\n",
    "            except:\n",
    "                print(f\"Movie {line[0]}/'{line[1]}' - Invalid year: {line[2]}\")\n",
    "                continue\n",
    "\n",
    "            # Convert IMDB rating to float\n",
    "            line[3] = float(line[3])\n",
    "\n",
    "            # Convert IMDB votes to int\n",
    "            line[4] = int(line[4])\n",
    "\n",
    "            # Strip off spaces, [], \\n, and '' from genres\n",
    "            line[5] = line[5].strip(\"\\n[]\").replace(\"'\", \"\").replace(\" \", \"\").split(\",\")\n",
    "\n",
    "            # Fields: movie ID, movie title, movie year, IMDB rating, number of IMDB votes, genres in the format ['genre1','genre2',ï¿½,'genreN']\n",
    "            movie_metadata |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"title\": line[1],\n",
    "                    \"year\": line[2],\n",
    "                    \"IMDB_rating\": line[3],\n",
    "                    \"IMDB_votes\": line[4],\n",
    "                    \"genres\": line[5],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(movie_metadata, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_character_metadata(movie_metadata_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Read in movie characters metadata.txt\n",
    "    character_metadata = {}\n",
    "    with open(\"data/movie_characters_metadata.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Strip off \"c\" from id and convert to int\n",
    "            line[0] = int(line[0][1:])\n",
    "\n",
    "            # Lowercase character name\n",
    "            line[1] = line[1].lower()\n",
    "\n",
    "            # Strip off m from movie id and convert to int, then ensure movie id is valid\n",
    "            line[2] = int(line[2][1:])\n",
    "            if line[2] not in movie_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Character - {line[0]}/'{line[1]}': Movie ID: {line[2]} does not exist in movie_titles_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Lowercase movie title\n",
    "            line[3] = line[3].lower()\n",
    "\n",
    "            # Convert gender to boolean (0 = male, 1 = female, ? = nan)\n",
    "            if line[4] == \"m\":\n",
    "                line[4] = False\n",
    "            elif line[4] == \"f\":\n",
    "                line[4] = True\n",
    "            elif line[4] == \"?\":\n",
    "                line[4] = np.nan\n",
    "\n",
    "            # Convert position to int and remove \"\\n\"\n",
    "            line[5] = line[5][:-1]\n",
    "            if line[5][-1] == \"?\":\n",
    "                line[5] = np.nan\n",
    "            else:\n",
    "                line[5] = int(line[5])\n",
    "\n",
    "            # Fields: character ID, character name, movie id, movie title, gender (\"?\" for unlabeled cases), position in credits (\"?\" for unlabeled cases)\n",
    "            character_metadata |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"name\": line[1],\n",
    "                    \"movie_id\": line[2],\n",
    "                    \"movie_title\": line[3],\n",
    "                    \"gender\": line[4],\n",
    "                    \"position\": line[5],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(character_metadata, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_data(\n",
    "    movie_metadata_df: pd.DataFrame, character_metadata_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Read in movie lines .txt\n",
    "    line_data = {}\n",
    "    with open(\"data/movie_lines.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Strip off \"L\" from line id and convert to int\n",
    "            line[0] = int(line[0][1:])\n",
    "\n",
    "            # Strip off \"c\" from character id and convert to int, then ensure character id is valid\n",
    "            line[1] = int(line[1][1:])\n",
    "            if line[1] not in character_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Line - {line[0]}: Character ID: {line[1]} does not exist in movie_characters_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Strip of \"m\" from movie id and convert to int, then ensure movie id is valid\n",
    "            line[2] = int(line[2][1:])\n",
    "            if line[2] not in movie_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Line - {line[0]}: Movie ID: {line[2]} does not exist in movie_titles_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Lowercase character name\n",
    "            line[3] = line[3].lower()\n",
    "\n",
    "            # Fields: line ID, character ID, movie id, character name, text\n",
    "            line_data |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"character_id\": line[1],\n",
    "                    \"movie_id\": line[2],\n",
    "                    \"character_name\": line[3],\n",
    "                    \"line\": line[4],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(line_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conversations_data(\n",
    "    movie_metadata_df: pd.DataFrame,\n",
    "    character_metadata_df: pd.DataFrame,\n",
    "    line_data_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    # Read in movie lines .txt\n",
    "    conversation_data = {}\n",
    "    with open(\"data/movie_conversations.txt\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            # Field separator: \" +++$+++ \"\n",
    "            line = line.split(\" +++$+++ \")\n",
    "\n",
    "            # Create a conversation index\n",
    "            line.insert(0, idx)\n",
    "\n",
    "            # Strip off \"c\" from both character id's and convert to int, then ensure character id is valid for both characters\n",
    "            line[1] = int(line[1][1:])\n",
    "            line[2] = int(line[2][1:])\n",
    "            if line[1] not in character_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Conversation - {line[0]}: Character ID: {line[1]} does not exist in movie_characters_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            if line[2] not in character_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Conversation - {line[0]}: Character ID: {line[2]} does not exist in movie_characters_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Strip off \"m\" from movie index and convert to int, then ensure movie id is valid\n",
    "            line[3] = int(line[3][1:])\n",
    "            if line[3] not in movie_metadata_df[\"id\"].values:\n",
    "                print(\n",
    "                    f\"Conversation - {line[0]}: Movie ID: {line[3]} does not exist in movie_titles_metadata.txt or has been removed.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Strip off spaces, L, [], \\n, and '' from lines, then ensure each line id is valid\n",
    "            line[4] = (\n",
    "                line[4].strip(\"\\n[]\").replace(\"'\", \"\").replace(\" \", \"\").replace(\"L\", \"\").split(\",\")\n",
    "            )\n",
    "            line[4] = [int(l) for l in line[4]]\n",
    "\n",
    "            invalid = False\n",
    "            for l in line[4]:\n",
    "                if l not in line_data_df[\"id\"].values:\n",
    "                    invalid = True\n",
    "\n",
    "            if invalid:\n",
    "                print(\n",
    "                        f\"Conversation - {line[0]}: A lineID it references does not exist in movie_lines.txt or has been removed.\"\n",
    "                    )\n",
    "\n",
    "            # Fields: first speaker character ID, second speaker character ID, movie ID, list of lines in chronological order ['lineID1', 'lineID2', ï¿½, 'lineIDN']\n",
    "            conversation_data |= {\n",
    "                idx: {\n",
    "                    \"id\": line[0],\n",
    "                    \"character ID 1\": line[1],\n",
    "                    \"character ID 2\": line[2],\n",
    "                    \"movie ID\": line[3],\n",
    "                    \"lines\": line[4],\n",
    "                }\n",
    "            }\n",
    "\n",
    "    return pd.DataFrame.from_dict(conversation_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 617\n",
      "Number of characters: 9035\n",
      "Number of lines: 304713\n",
      "Number of conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "# Final report on data\n",
    "movie_metadata_df = read_movie_metadata()\n",
    "character_metadata_df = read_character_metadata(movie_metadata_df)\n",
    "line_data_df = read_line_data(movie_metadata_df, character_metadata_df)\n",
    "conversation_data_df = read_conversations_data(movie_metadata_df, character_metadata_df, line_data_df)\n",
    "\n",
    "print(f\"Number of movies: {len(movie_metadata_df)}\")\n",
    "print(f\"Number of characters: {len(character_metadata_df)}\")\n",
    "print(f\"Number of lines: {len(line_data_df)}\")\n",
    "print(f\"Number of conversations: {len(conversation_data_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df = movie_metadata_df\n",
    "character_df = character_metadata_df\n",
    "line_df = line_data_df\n",
    "conversation_df = conversation_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 617\n",
      "Number of characters: 9035\n",
      "Number of lines: 304713\n",
      "Number of conversations: 83097\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of movies: {len(movie_df)}\")\n",
    "print(f\"Number of characters: {len(character_df)}\")\n",
    "print(f\"Number of lines: {len(line_df)}\")\n",
    "print(f\"Number of conversations: {len(conversation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = conversation_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conversation(conversation_data, line_df):\n",
    "    # Turn conversation into chat format of one input and one response\n",
    "    # For each pair of lines, add start of sentence token, end of sentence token, and bot response\n",
    "\n",
    "    chats = []\n",
    "    for data in conversation_data:\n",
    "        # If it is not even, drop the last line as it is missing the bot response\n",
    "        if len(data) % 2 == 1:\n",
    "            data = data[:-1]\n",
    "\n",
    "        for idx, line in enumerate(data):\n",
    "            line_text = line_df[line_df[\"id\"] == line][\"line\"].values[0]\n",
    "\n",
    "            if idx % 2 == 0:\n",
    "                chat = \" \".join([\"<SOS>\", line_text])\n",
    "            else:\n",
    "                chat = \" \".join([chat, \"<BOT>\", line_text, \"<EOS>\"])\n",
    "                chats.append(chat)\n",
    "\n",
    "    return chats\n",
    "\n",
    "\n",
    "chats = convert_to_conversation(conversation_df[\"lines\"].values, line_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138135\n"
     ]
    }
   ],
   "source": [
    "print(len(chats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", clean_up_tokenization_spaces=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add special tokens for start/end of sentence, padding, and bot response\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\", \"bos_token\": \"<SOS>\", \"eos_token\": \"<EOS>\"})\n",
    "tokenizer.add_tokens([\"<BOT>\"])\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\seanp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Hi how are you?  <BOT>  <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(**tokenizer(\"<SOS> Hi how are you? <BOT> \", return_tensors=\"pt\"))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, chats, tokenizer):\n",
    "        self.chats = chats\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 128\n",
    "\n",
    "        self.encoded_data = self.tokenizer(self.chats, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n",
    "        self.input_id = self.encoded_data[\"input_ids\"]\n",
    "        self.attention_mask = self.encoded_data[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chats)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_id[idx], self.attention_mask[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50261, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from better_profanity import profanity\n",
    "import re\n",
    "\n",
    "\n",
    "def train_model(chat_data, model, optimizer, device, epochs=10, save_every=5):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch = 0\n",
    "        for input_id, attention_mask in chat_data:\n",
    "            batch += 1\n",
    "            # print(f\"Batch: {batch}\")\n",
    "            input_id = input_id.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            output = model(input_id, attention_mask=attention_mask, labels=input_id)\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Print example chat\n",
    "        # example = tokenizer(\"<SOS> Hi I am Ethan, how are you? <BOT>\", return_tensors=\"pt\")\n",
    "        # input_id = example[\"input_ids\"].to(device)\n",
    "        # attention_mask = example[\"attention_mask\"].to(device)\n",
    "        # print(tokenizer.decode(model.generate(input_id, attention_mask=attention_mask)[0]))\n",
    "\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save(model.state_dict(), Path(f\"models/gpt2/checkpoints/model_{epoch}.pt\"))\n",
    "\n",
    "    torch.save(model.state_dict(), Path(f\"models/gpt2/final/model_final.pt\"))\n",
    "\n",
    "def truncate_history(history, tokenizer, max_length=128):\n",
    "    # Tokenizing history\n",
    "    encoded_history = [tokenizer.encode(entry, return_tensors=\"pt\")[0] for entry in history]\n",
    "\n",
    "    combined_history = torch.cat(encoded_history, dim=0)\n",
    "\n",
    "    # If history exceeds max_length, truncate\n",
    "    if len(encoded_history) >= max_length:\n",
    "        truncated_history = encoded_history[-max_length:]\n",
    "    else:\n",
    "        truncated_history = combined_history\n",
    "\n",
    "    truncated_history_ids = truncated_history.tolist()\n",
    "\n",
    "    truncated_text = tokenizer.decode(truncated_history_ids, skip_special_tokens=False)\n",
    "\n",
    "    return truncated_text.strip()\n",
    "\n",
    "def generate_context(chat_history, max_entries=2):\n",
    "    # Generating context based off of history\n",
    "    return list(chat_history)[-max_entries:]\n",
    "\n",
    "def inference(model, tokenizer, device):\n",
    "    model.eval()\n",
    "    chat_history = deque(maxlen=5)\n",
    "    chat = input(\"User: \")\n",
    "    \n",
    "    while chat.lower() != \"quit\" and chat != \"q\":\n",
    "        user_input = f\"<SOS> {chat.strip()}\"\n",
    "        chat_history.append(user_input)   # Adding user input to history\n",
    "        \n",
    "        context = generate_context([entry for entry in chat_history if \"<SOS>\" in entry]) # Generating context (not including most recent user input)    context, tokenizer, max_length=128) # Truncating user history so it doesn't exceed max_length of 128 in longer convo\n",
    "        truncated_history = truncate_history(context, tokenizer, max_length=128) # Truncating user history so it doesn't exceed max_length of 128 in longer convo\n",
    "        \n",
    "        chat_encoded = tokenizer(truncated_history, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "        input_id = chat_encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = chat_encoded[\"attention_mask\"].to(device)\n",
    "        \n",
    "        output = model.generate(\n",
    "            input_id, \n",
    "            attention_mask=attention_mask, \n",
    "            min_length=30,  # Setting minimum new tokens to ensure responses aren't too short\n",
    "            max_new_tokens=100, # Setting maximum new tokens to ensure responses aren't too verbose\n",
    "            do_sample=True, # Set to true for use with temp, top-k, and top-p\n",
    "            temperature=0.2, # Lowering temperature for more coherent responses.\n",
    "            top_k=50, # Adding top-k sampling to limit to top 50 likely next tokens.\n",
    "            top_p=0.9, # Adding nucleus sampling as well.\n",
    "            repetition_penalty=1.5, # Penalizing repeated sequences\n",
    "            pad_token_id=tokenizer.eos_token_id # Handling end of sequence properly\n",
    "        )   \n",
    "        \n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "        # Removing previous responses and special tokens from output \n",
    "        if response.startswith(user_input):\n",
    "            response = response[len(user_input):].strip()\n",
    "        elif response.startswith(chat_history[-2] + user_input):\n",
    "            response = response[len(chat_history[-2] + user_input):].strip()\n",
    "        response = response.replace(\"<BOT>\", \"\").replace(\"<SOS>\", \"\").replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").strip()\n",
    "\n",
    "        # Censoring profanity\n",
    "        def clean_response(response):\n",
    "            return profanity.censor(response)\n",
    "\n",
    "        cleaned_response = clean_response(response)\n",
    "\n",
    "        # Post-processing to truncate at 70 tokens or nearest sentence end\n",
    "        max_length = 70\n",
    "        tokens = cleaned_response.split()\n",
    "        truncated_response = \" \".join(tokens[:max_length])\n",
    "\n",
    "        sentence_endings = list(re.finditer(r'[.!?]', cleaned_response))\n",
    "            \n",
    "        if not sentence_endings:\n",
    "            truncated_response = truncated_response.strip()\n",
    "        else:\n",
    "            for match in sentence_endings:\n",
    "                next_cut = match.end()\n",
    "                possible_truncation = cleaned_response[:next_cut].strip()\n",
    "                \n",
    "                token_count = len(possible_truncation.split())\n",
    "                if token_count <= max_length:\n",
    "                    truncated_response = possible_truncation\n",
    "                else:\n",
    "                    break     \n",
    "\n",
    "        print(f\"Bot: {truncated_response}\")\n",
    "\n",
    "        chat = input(\"User: \")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "chat_data = DataLoader(ChatData(chats[0:16384], tokenizer), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 20/20 [56:40<00:00, 170.04s/it]\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "train_model(chat_data, model, optimizer, device, epochs=20, save_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  What is the meaning of life?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Lives are ephemeral. Things we don't live with anymore... like art or music... you can only imagine what it must have meant to those around them when they died, before their time -- all dead and forgotten as a whole in our lifetimes! I'm not gonna pretend there's absolutely no way this means anything other than making things better for ourselves..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Wow that's pretty dismal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Well, it seems we've got some sorta out there... a search for answers here on this alien planet.. and maybe even an answer to our own existence as we know It! Now what kindof questions does one ask ourselves when faced with such problems...? Do they have any knowledge at all about anything like.... well perhaps... astrology???\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Yeah Earth does seem kind of alien in some places, huh?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Yes... it is.. depressing really! I don't know why they keep this on their birth certificates but surely they must have thought something awful was going to happen here somewhere down there...and probably killing people as well.... Anyway the aliens killed a lot more than were injured and tended animals when coming out from hibernation so by examining them now you'd say...they've re-engineered our DNA somehow?!!! What about \"we\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User:  Who's re-engineered our DNA?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Dr. Weir... I'm not sure what you're talking about -- why we need the ship or anything else besides life support...? Starck: Where are your people now?! How many colonists have they left on board!? What happened to them??? Commander Alexander Hamilton has yet again come up with a replacement for this starboard computer! Why haven't those men ever retired from active duty??\n"
     ]
    }
   ],
   "source": [
    "inference(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
